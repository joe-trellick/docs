"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8351],{3905:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return u}});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function r(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=n.createContext({}),h=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},d=function(e){var t=h(e.components);return n.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,d=r(e,["components","mdxType","originalType","parentName"]),p=h(a),u=i,m=p["".concat(l,".").concat(u)]||p[u]||c[u]||o;return a?n.createElement(m,s(s({ref:t},d),{},{components:a})):n.createElement(m,s({ref:t},d))}));function u(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=a.length,s=new Array(o);s[0]=p;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r.mdxType="string"==typeof e?e:i,s[1]=r;for(var h=2;h<o;h++)s[h]=a[h];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},30018:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return r},contentTitle:function(){return l},metadata:function(){return h},toc:function(){return d},ImageHolder:function(){return c},default:function(){return u}});var n=a(87462),i=a(63366),o=(a(67294),a(3905)),s=["components"],r={title:"Big Peer Internals"},l=void 0,h={unversionedId:"advanced/architecture/big-peer-internals",id:"advanced/architecture/big-peer-internals",isDocsHomePage:!1,title:"Big Peer Internals",description:"export function ImageHolder(props) {",source:"@site/docs/advanced/architecture/big-peer-internals.md",sourceDirName:"advanced/architecture",slug:"/advanced/architecture/big-peer-internals",permalink:"/advanced/architecture/big-peer-internals",editUrl:"https://github.com/getditto/docs/edit/master/docs/advanced/architecture/big-peer-internals.md",tags:[],version:"current",frontMatter:{title:"Big Peer Internals"},sidebar:"tutorialSidebar",previous:{title:"Mesh Network",permalink:"/advanced/architecture/mesh-network"},next:{title:"Certificate Based Security",permalink:"/advanced/certificate-security"}},d=[{value:"What Is It?",id:"what-is-it",children:[]},{value:"Why Did You Make It?",id:"why-did-you-make-it",children:[]},{value:"How Does It Work?",id:"how-does-it-work",children:[{value:"Ditto CRDTs",id:"ditto-crdts",children:[]},{value:"Ditto Mesh Replication",id:"ditto-mesh-replication",children:[]},{value:"Apps and Collections",id:"apps-and-collections",children:[]},{value:"Causally Consistent Transactions",id:"causally-consistent-transactions",children:[]},{value:"PaRiS - UST",id:"paris---ust",children:[]},{value:"The Log",id:"the-log",children:[]},{value:"Storage Nodes",id:"storage-nodes",children:[]}]},{value:"Transitions",id:"transitions",children:[{value:"Backfill, again",id:"backfill-again",children:[]},{value:"Routing, and a UST per-Configuration",id:"routing-and-a-ust-per-configuration",children:[]}]},{value:"Handling Failure",id:"handling-failure",children:[{value:"Bad Nodes",id:"bad-nodes",children:[]},{value:"Missed/Lost Data",id:"missedlost-data",children:[]},{value:"Subscription Servers",id:"subscription-servers",children:[]}]},{value:"What Next?",id:"what-next",children:[{value:"CDC",id:"cdc",children:[]},{value:"Time Series",id:"time-series",children:[]},{value:"HTTP API",id:"http-api",children:[]},{value:"Eventual Consistency Mode",id:"eventual-consistency-mode",children:[]},{value:"Improved Queries and Indexing",id:"improved-queries-and-indexing",children:[]},{value:"Summary",id:"summary",children:[]}]}];function c(e){return(0,o.kt)("div",{style:{padding:"2rem",margin:"2rem",borderRadius:"8px",background:"white"}},e.children)}var p={toc:d,ImageHolder:c};function u(e){var t=e.components,r=(0,i.Z)(e,s);return(0,o.kt)("wrapper",(0,n.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"big-peer-internal-architecture---hydra"},"Big Peer Internal Architecture - HyDRA"),(0,o.kt)("p",null,"HyDRA is Ditto's cloud database. It is designed to work in conjunction\nwith the Ditto SDK, and also has an HTTP API."),(0,o.kt)("h2",{id:"what-is-it"},"What Is It?"),(0,o.kt)("p",null,'HyDRA fits into Ditto\'s vision of syncing data, anywhere. HyDRA is cloud-ready, multi-tenant, highly available, fault\ntolerant, offers causally consistent transactions, and works\nseamlessly with Ditto SDK mesh devices as a "big peer".'),(0,o.kt)("h2",{id:"why-did-you-make-it"},"Why Did You Make It?"),(0,o.kt)("p",null,"Even with the Ditto SDK some pair of devices may not be able to\nexchange data. Maybe they are miles apart, or they are never online at\nthe same time. That is where HyDRA fits in. HyDRA is a database that\nDitto SDK mesh devices can sync with to propagate changes across\ndisconnected meshes, and even back to the enterprise. So often\ndatabases are used as channels, which is also one of HyDRA's\npurposes."),(0,o.kt)("p",null,"There exist many distributed databases, but HyDRA is specifically\ndesigned for Ditto: It stores Ditto's CRDTs by default; it can store\nand merge Ditto ",(0,o.kt)("a",{parentName:"p",href:"https://ditto.live/docs/architecture/crdt"},"CRDT"),' Diffs; it "speaks"\nDitto\'s mesh replication protocol, meaning it appears as just another\npeer to Ditto mesh devices; and it provides causally consistent\ntransactions.'),(0,o.kt)("h2",{id:"how-does-it-work"},"How Does It Work?"),(0,o.kt)("p",null,'HyDRA is made up of core storage nodes which make a distributed database, and\nsoft-state satellite API nodes, called Subscription Servers, that are also\ncaches of data and replicate with Ditto SDK Mesh clients as a "Big Peer".'),(0,o.kt)("p",null,"The following sections go into detail about what properties and\nfeatures HyDRA has, and how we achieve those properties, leveraging\nour experience building and shipping distributed databases, and\ncurrent computer science systems research."),(0,o.kt)("p",null,"The following drawing is a rough overview of the architecture."),(0,o.kt)(c,{mdxType:"ImageHolder"},(0,o.kt)("p",null,(0,o.kt)("img",{alt:"HyDRA Overview",src:a(8059).Z}))),(0,o.kt)("h3",{id:"ditto-crdts"},"Ditto CRDTs"),(0,o.kt)("p",null,"The core data type in Ditto is the CRDT. It is documented in detail\n",(0,o.kt)("a",{parentName:"p",href:"https://ditto.live/docs/architecture/crdt"},"here"),". Understanding some\nof how the CRDT works helps understand the concepts below.  It is\nenough to know that if the same CRDT is modified by multiple Ditto\nmesh SDK devices concurrently there is a way to deterministically\n",(0,o.kt)("em",{parentName:"p"},"merge")," the conflicting versions into a single meaningful value."),(0,o.kt)("h3",{id:"ditto-mesh-replication"},"Ditto Mesh Replication"),(0,o.kt)("p",null,"This is also covered in ",(0,o.kt)("a",{parentName:"p",href:"https://ditto.live/docs/architecture/mesh-network"},"other documents"),'. All we need know\nhere is that Ditto Mesh SDK devices replicate with HyDRA by sending\nCRDT Documents, and CRDT Diffs to HyDRA\'s Subscription Server API, and\nreceive in return Documents and Diffs that they are subscribed to. A\nsubscription is a query, for example "All red cars in the vehicles\ncollection."'),(0,o.kt)("p",null,"Thanks to the Ditto replication protocol all Documents and Diffs that\nthe client needs to send/receive to/from HyDRA appear to arrive\natomically, as though in a transaction."),(0,o.kt)("h3",{id:"apps-and-collections"},"Apps and Collections"),(0,o.kt)("p",null,"An application is the consistency boundary for HyDRA. An application\nis registered via the ",(0,o.kt)("a",{parentName:"p",href:"https://portal.ditto.live/"},"Portal"),". An application is uniquely\nidentified via association with a UUID. Queries, Subscriptions, and\nTransaction are all scoped by application. Within an application are\nCollections. Theses are somewhat like tables, where associated\nDocuments can be stored. HyDRA supports transactions within an Application, including across\nCollections."),(0,o.kt)("h3",{id:"causally-consistent-transactions"},"Causally Consistent Transactions"),(0,o.kt)("p",null,"Given the existence of the CAP theorem, that fundamental trade off in\ndistributed systems between Consistency and Availability in a world of\nasynchronous networks, Causal Consistency is the strongest consistency model\nthat can be achieved if a system is designed to continue to be Available in the\nCAP sense. You can read more\n",(0,o.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Causal_consistency"},"on Wikipedia"),". "),(0,o.kt)("p",null,"Causal Consistency is a model that is much simpler to work with, compared to\neventual consistency. In eventual consistency it seems like ",(0,o.kt)("em",{parentName:"p"},"anything")," is\nallowed to happen. With Causal Consistency if one action happens before another,\nand can therefore potentially influence that other action, the actions must be\nordered in the that way for everyone, ever after. If two actions are totally\nunrelated, they can be ordered any way the system chooses. By way of example:"),(0,o.kt)("p",null,"Imagine that you have two collections: Followers and Pictures. Before\nadding your holiday snaps to Pictures, you remove your boss from\nFollowers. If these two independent actions were re-ordered by an\neventually consistent system, bad things could happen. Causal\nConsistency ensures that the boss is removed ",(0,o.kt)("em",{parentName:"p"},"before")," the pictures are\nvisible, regardless of the vagaries of networks, connections, and\nordering of messages. Transactional Causal Consistency means that we\ncan apply this constraint across any number of related changes, across\nmultiple documents, in multiple collections, as long as they are within the same\nApplication. This is a much simpler to understand model than eventual\nconsistency, leading to fewer surprises."),(0,o.kt)("h3",{id:"paris---ust"},"PaRiS - UST"),(0,o.kt)("p",null,"This section gets technical on ",(0,o.kt)("em",{parentName:"p"},"how")," HyDRA provides Causally Consistent\nTransactions, and other properties, like fault tolerance, and scalability."),(0,o.kt)("p",null,"The key concept throughout, and the primitive on which HyDRA is built is that of\nthe UST, the Universally Stable Timestamp. Along with some core architecture,\nthe UST is inspired by the paper ",(0,o.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1902.09327"},"PaRiS: Causally Consistent Transactions with\nNon-blocking Reads and Partial Replication"),".\nThe paper describes a system that very closely matches Ditto's needs. The system\nis a database, one that is partitioned (or sharded) to allow storage of a great\ndeal of data, and replicated to provide fault tolerance (and better tail\nlatencies/work distribution.) PaRiS supports non-blocking reads in the\npast, and causally consistent transactions. The key ingredient is the UST."),(0,o.kt)("p",null,"In PaRiS every write transaction is given a unique timestamp. All\ntransactions that contain data for the same partitions will have a\ntimestamp that is ordered causally, non-intersecting transactions can\nhave equal timestamps, as they have no causal relationship/order."),(0,o.kt)("p",null,'The key concept is that Transactions are Ordered by Timestamp. Changes that have\na Causal relationship express their order relationship through the order of\ntransaction timestamps. Transactions with no causal relationship can be ordered\nin any way. In the example above, as the change to the "Followers" collections\nhappens before the changes to the "Pictures" collection. The first would have a\nlower transaction ID than the second (if not part of the ',(0,o.kt)("em",{parentName:"p"},"same")," transaction.)"),(0,o.kt)("p",null,"Before going into more details about Ditto's implementation, some clarification\non terms and concepts follows."),(0,o.kt)("h4",{id:"replicas"},"Replicas"),(0,o.kt)("p",null,"Replicas are independent copies of the same data. This is an aid to fault tolerance\nand performance. If you have 3 replicas and a disk fails, you still have two\ncopies. Or if one or two replicas are unreachable due to network conditions, you\ncan still read from a reachable one. Replicas are also useful to provide more\ncapacity to serve reads. If you have three replicas you can balance reads across\nall three, each doing a third of the work. How data is replicated has an effect\non when you can read what."),(0,o.kt)("p",null,"As an initial look at the UST, imagine a database on a single machine, with a\ntransaction log. Each transaction to be written goes onto the log and is given a\nsequence number. When the transaction is committed the sequence number can be\nthought of as the current version of the database. When transaction with\nsequence number 1 is committed, the database is at version 1. When you read the\ndatabase after transaction 1 is committed you are reading version 1. When the\n2nd transaction commits, the database is at version 2, and so on."),(0,o.kt)("p",null,"If we now wish to have two replicas of our data, how we replicate matters. If\none replica is the primary, and another is the secondary*, maybe the primary\ncommits transaction 1, and then sends it to the secondary, who also commits it.\nNow the database is at version 1, and whichever replica you read from you get\nthe same answer. But what if transaction two doesn't make it to the secondary?\nThere is a brief network outage, or for some reason the message is delayed. The\nprimary has committed transactions 1 and 2, but the secondary has only committed"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"})),(0,o.kt)("p",null,"If the system wishes to spread the read load equally, and a client reads from\nthe primary, and after that the secondary, they will see a weird view of the\nworld, where time goes backwards between the first read, and the second.\nHowever, we could decide that since only version 1 is committed on both nodes,\nthen the version of the database could be thought of as version 1. This is the\nhighest transaction that is committed on both replicas: the universally stable\ntimestamp. A client reading from either replica will get a consistent view of\nthe data."),(0,o.kt)("p",null,"(*We don't have Primaries and Secondaries in HyDRA, I used them to motivate the\nexample. In HyDRA all the database nodes are equal \u270a.)"),(0,o.kt)("h4",{id:"versions"},"Versions"),(0,o.kt)("p",null,'The above scenario in "replicas" suggests that we need to keep multiple versions\nof our data. If Transaction 1 changes documents A, B, and C, and Transaction 2\nchanges documents A, C, and F, BUT only one replica has stored both\nTransactions, then the database is at version 1. We therefore need to have the\ndata for A and C at Transaction 1 and 2, since if we want to provide a\nconsistent view of the data (one that does not go back in time) then we can only\nserve reads as of version 1 at first, and then later as of version 2.'),(0,o.kt)("p",null,"HyDRA keeps as many versions of each data item as it needs in order to provide\nconsistent reads. If this concerns you, skip ahead to garbage collection."),(0,o.kt)("p",null,'Note that HyDRA uses Ditto CRDTs as the data type, meaning all\nversions can be deterministically collapsed into one version, by\nmerging the CRDTs. In some cases a "version" is in only a ',(0,o.kt)("inlineCode",{parentName:"p"},"Diff")," and not a whole document."),(0,o.kt)("h4",{id:"partitions--shards"},"Partitions / Shards"),(0,o.kt)("p",null,"In order to evolve the conceptual model we can add in partitioning of\nthe data, or partial replication as it is called in the PaRiS\npaper. Often called Sharding, this is the practice of splitting up the\nkey space of a database, and assigning a subset of it to different\nservers. See Random Slicing below for details of HOW we do this in\nHyDRA."),(0,o.kt)("p",null,"Now we have replicas of the data, and we partition the data. Each storage node\nin HyDRA is responsible for one replica of a data partition. If we want to split\nour data across three partitions, and have 2 replicas of each item, then we can\ndeploy 6 servers, 2 in each partition."),(0,o.kt)("p",null,'Returning to our example in Causal Consistency, imagine that the documents in\nthe "Followers" Collection is stored in Partition 1, and the "Pictures"\nCollection in Partition 2 and that the change to Followers and pictures occurs\nin the same transaction, Transaction One.'),(0,o.kt)("p",null,"This transaction contains documents that are stored in 2 different\npartitions, across a total of 4 locations (2 replicas, 2 partitions.)"),(0,o.kt)("p",null,'In order to store the data for this transaction it needs to stored on\nall 4 servers. This is why the UST matters. If, by chance,\nHyDRA stores the "Pictures" change document ',(0,o.kt)("em",{parentName:"p"},"before"),' storing the\n"Followers" change document and allow reads to always get the latest\nvalue, we can break the consistency constraint, and show the boss our\nholiday pictures.'),(0,o.kt)("p",null,"A more complex example:"),(0,o.kt)("p",null,"If we have 4 transactions in flight, maybe all the servers have committed\ntransaction one, half have committed transaction two, all have committed three,\nand only 2 servers have committed transaction four. If we want to have\nconsistent read of the data, we have to read at the version that is stable at\nall servers: transaction one. Note: we can't say that Transaction Three is\nstable, since it follows Transaction Two, which is not yet stable. Causal\nConsistency is all about the order of updates."),(0,o.kt)("h4",{id:"non-blocking-reads"},"Non-Blocking Reads"),(0,o.kt)("p",null,"When reading from HyDRA you don't have to wait for the last write to\nbecome stable before reading. Instead, HyDRA is always able to return a\nversion of the data for the UST. Reading in the past is still causally\nconsistent, and it means that reads and writes proceed\nindependently. It also means that something is always available to be\nread (given one replica per-partition is reachable.) A reasonable\ntrade-off."),(0,o.kt)("h4",{id:"read-your-own-writes"},"Read Your Own Writes"),(0,o.kt)("p",null,"In the PaRiS paper, the database clients must have a local cache of\ntheir own writes, so that they can always read their own writes. In\nDitto, the SDK Mesh clients are fully fledged partial replicas of the\ndatabase, and can ",(0,o.kt)("em",{parentName:"p"},"always")," read their own writes. For the HTTP API,\nwrites return a Timestamp at which the write is visible. HTTP Read\nrequest can provide this timestamp to ensure Read-Your-Own-Writes\nsemantics."),(0,o.kt)("h3",{id:"the-log"},"The Log"),(0,o.kt)("p",null,"A core concept in HyDRA is the log. We use a transaction log to\npropagate updates to the database. In PaRiS a two-phase commit process\nis used to negotiate an ",(0,o.kt)("a",{parentName:"p",href:"https://cse.buffalo.edu/tech-reports/2014-04.pdf"},"HLC")," based sequence\nnumber for each transaction. In HyDRA we use the log to sequence\ntransactions. The sequence number for a Transaction on the log becomes\nthe transaction timestamp, and transaction timestamps are what the UST\nreflects. The Transaction Timestamps in HyDRA form a total sequence,\nfrom ZERO (initial empty database version) on up.  Each storage node\nconsumes from the log, and a transaction is stable when all nodes\nhave observed the transaction, those that own data in the\ntransaction having written that data durably."),(0,o.kt)("p",null,"At present our log is Kafka, as it suits our needs well. Though Kafka\nis at the heart of HyDRA it is not a core architectural feature: any\nlog will do. At present, we use a single partition of a single topic,\nbut we can partition the log by Application and still maintain the\nsame consistency guarantees. When we do partition the log the\nproperties are the same, the throughput increases, and the UST becomes\na vector."),(0,o.kt)("h3",{id:"storage-nodes"},"Storage Nodes"),(0,o.kt)("p",null,"HyDRA is split into Storage Nodes and Subscription Servers. The Storage Nodes\nare the database nodes, they run RocksDb as a local storage engine. A storage\nnode consumes the transaction log, commits data to disk, and gossips with the\nother storage nodes."),(0,o.kt)("h4",{id:"gossip---ust"},"Gossip - UST"),(0,o.kt)("p",null,"Each node gossips the highest transaction that it has committed. From\nthis gossip any node can calculate what it considers to be the UST. If\nevery server gossips its local MAXIMUM committed transaction, then the\nUST is the MINIMUM of those MAXIMUMS. For example, in a 3 node\ncluster:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"server 1 has committed Txn 10"),(0,o.kt)("li",{parentName:"ul"},"server 2 has committed Txn 5"),(0,o.kt)("li",{parentName:"ul"},"server 3 has committed Txn 7")),(0,o.kt)("p",null,'The UST is "5".'),(0,o.kt)("p",null,"NOTE: that each server can have a different ",(0,o.kt)("em",{parentName:"p"},"view")," of the UST, depending on how\nlong it takes messages to be passed around. For example:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},'server 1 has committed Txn 10, and has heard from server 2 that it has\ncommitted Txn 4, and from Server 3 that it has committed Txn 6. Server 1\nthinks the UST is "4".'),(0,o.kt)("li",{parentName:"ul"},'server 2 has committed Txn 5 and has heard from server 1 that it has committed\nTxn 7, and from Server 3 that it has committed Txn 6. Server thinks the UST is\n"5"'),(0,o.kt)("li",{parentName:"ul"},'server 3 has committed Txn 7 and has heard from server 1 that it has committed\nTxn 9, and from Server 2 that it has committed Txn 3. Server thinks the UST is\n"3"')),(0,o.kt)("p",null,"But whatever the view of the UST, it reflects a causally consistent version of\nthe database that can be read."),(0,o.kt)("p",null,"When HyDRA is working, then the UST moves up. When HyDRA is quiescent the UST will\nbe the same on every node, and will reflect the last transaction produced by the\nlog."),(0,o.kt)("p",null,"The mechanism for gossip in HyDRA is the subject of future optimization work."),(0,o.kt)("h4",{id:"gossip---garbage-collection"},"Gossip - Garbage Collection"),(0,o.kt)("p",null,"Very similar to the UST is the Garbage Collection Timestamp. It works\nclosely with Read Transactions (below). The Cluster GC Timestamp\nrepresents the lowest Transaction Timestamp that must not be garbage\ncollected. The GC timestamp and the UST form a sliding window of\nversions over the database that represent the Timestamp versions at\nwhich a Causally Consistent query can be executed."),(0,o.kt)("p",null,"Document versions below the GC Timestamp can be garbage\ncollected. Garbage Collection is a periodic process that scans some\nsegment of the database, and rolls up, or merges all the versions\nbelow the GC timestamp, re-writing them as a single value. Thanks to\nDitto CRDTs this leads to deterministic outcome value for each\ndocument at each version."),(0,o.kt)("p",null,"Garbage Collection keeps the number of versions to a minimum, making\nreads more efficient, and reclaiming disk space."),(0,o.kt)("p",null,"The Garbage Collection Timestamp is calculated as the minimum active\nRead Transaction Timestamp across the cluster."),(0,o.kt)("h4",{id:"reading-and-read-transactions"},"Reading and Read Transactions"),(0,o.kt)("p",null,"Queries are handled by a coordinating node. Any node can coordinate a query, because every node has a local copy of the Partition Map, from the Cluster Configuration. So the coordinator can be chosen at random, or via some other load balancing heuristic."),(0,o.kt)("p",null,"The node will look at the\nquery and decide which partitions contain the data needed to answer\nthe query. At present, HyDRA shards data by Application AND Collection\n(however this can change in the future.) The coordinator will assemble a\nlist of partitions needed to answer the query, and pick one replica\nfrom each partition. It picks the replica based on a\n",(0,o.kt)("a",{parentName:"p",href:"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.581.8294&rep=rep1&type=pdf"},"fault-detector"),",\npicking the replica least likely to be faulted. It sends the query to\neach replica, and merges and streams the results back to the caller."),(0,o.kt)("p",null,"The Coordinator issues the query to each partition with a predetermined Timestamp.\nThis timestamp is usually the UST at the Coordinator but can be any Timestamp between the cluster Garbage Collection Timestamp and the UST."),(0,o.kt)("p",null,"When a node coordinates a Read Transaction it locally holds some metadata in memory of the UST at which the Read began. This data is used to calculate  the Local Garbage Collection Timestamp that the node gossips. The Local GC timestamp is the maximum transaction below the minimum read transaction. The GC timestamp proceeds monotonically upwards, as does the UST. When the query is complete, the Read Transaction is removed from memory, and the GC timestamp can rise."),(0,o.kt)("p",null,"A node that is not currently performing a Read Transaction will still gossip its view of the UST as the GC timestamp. This way progress can always be made."),(0,o.kt)("p",null,"In a quiescent cluster with no reads, the GC timestamp will equal the UST, and there will be exactly one version of each data item."),(0,o.kt)("h4",{id:"cluster-configurations-who-owns-what"},"Cluster Configurations: Who owns what?"),(0,o.kt)("p",null,"The details of the cluster: its size, shape, members, partitions,\nreplicas etc. are all encapsulated in a Cluster Configuration. When\nthere is a need to change a cluster we create a new Cluster\nConfiguration and instruct HyDRA to transition from the Current\nConfiguration to the Next Configuration."),(0,o.kt)("p",null,"Everything discussed so far describes a static configuration of\npartitions and replicas. However, clusters must scale up and down, and\nfaulty nodes must be removed and replaced. HyDRA must support dynamic\nscaling without downtime, and it must do so while maintaining Causal\nConsistency, always accepting writes and serving reads."),(0,o.kt)("p",null,"Ideally when a cluster is changed, there should be minimal data movement EG. if\nwe grow the cluster, we want to only move the minimum amount of data necessary\nto the new nodes."),(0,o.kt)("p",null,"Before discussing Transitions in detail, it's helpful to look at how data is\nplaced in a HyDRA cluster, and for that we use Random Slicing."),(0,o.kt)("h4",{id:"random-slicing"},"Random Slicing"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"ftp://ftp.cse.ucsc.edu/pub/darrell/miranda-tos14.pdf"},"Random Slicing")," has been written about brilliantly in ",(0,o.kt)("a",{parentName:"p",href:"https://www.infoq.com/articles/dynamo-riak-random-slicing/"},"this\narticle")," by Scott Lystig-Fritchie, which motivates the WHY of Random\nSlicing as well as explaining the HOW. Here I'm going to briefly discuss\nHyDRA's implementation."),(0,o.kt)("p",null,"We made a decision to make this first version of HyDRA as simple as\npossible, and so we elected to keep our cluster shape and replica\nplacement very simple (though it is extensible and will get richer as\ntime allows or needs dictate.)"),(0,o.kt)("p",null,'Each document in HyDRA has a key, or document ID which is made up of a\nNamespace (The Application (AppId) and the\ncollection) and an ID for the document. We hash a portion of this\nDocumentId (at present the Namespace) and that gives us a number. This\nnumber decides in which partition the data item lives. Our current\nhashing policy has the effect that data in the same Collection is\nco-located in the same partition, which makes queries in a single\nCollection more efficient. It may also lead to hot spots, but this can\nbe mitigated by either hashing more of the DocumentId (to split\nCollections), or inserting a layer of indirection that allows us to\nmap hot partitions to bigger nodes ("The Bieber problem": see the\npaper, or Scott\'s article for details.)'),(0,o.kt)("p",null,"As per the Random Slicing algorithm, we think of the keyspace as the range 0 to"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"We take the ",(0,o.kt)("em",{parentName:"li"},"capacity")," of the cluster, and  divide 1 by it. This determines\nhow much of the keyspace each partition owns.")),(0,o.kt)("p",null,"In our initial, naive, implementation the capacity is the number of partitions we\nwish to have. We enforce an equal number of replicas per-partition, and thus all\nclusters are rectangular. E.g. 1",(0,o.kt)("em",{parentName:"p"},"1, or 2"),"3, or 5*2, etc., where the first\nnumber is the number of partitions, and the second the number of replicas.\nRandom Slicing allows in future to have heterogeneous nodes, assigning the\ncapacity accordingly."),(0,o.kt)("p",null,"In the case that we want 3 partitions of 2 replicas, we say each\npartition takes 1/3 of the keyspace, or has 1/3 of the capacity."),(0,o.kt)("p",null,"Hashing a DocumentId then gives us a number that falls into the 1st,\n2nd or 3rd 1/3 of the keyspace, and that decides which partition\n",(0,o.kt)("em",{parentName:"p"},"owns")," that document."),(0,o.kt)("p",null,"We can transition from any configuration to any other, and we do this by slicing\nor coalescing partitions using the Cut-Shift algorithm from the Random Slicing\npaper."),(0,o.kt)("p",null,"The graphic below illustrates how this looks."),(0,o.kt)(c,{mdxType:"ImageHolder"},(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Random Slicing Cutshift",src:a(2928).Z}))),(0,o.kt)("p",null,"As the image shows, Partition Four is made up of slices from P1, P2, and P3,\nthese three slices we call Intervals. They represent, in this case, two disjoint\nranges of the keyspace that P4 owns. A replica of P4 has two intervals, whereas\nP1 has a contiguous range and a single interval."),(0,o.kt)("p",null,"Our Random Slicing implementation is currently limited in that resources must be\nadded and removed in the cluster in units equal to the desired replication\nfactor. If you want to add a node, and your desired replication factor is 2, you\nmust add 2 nodes. This is not a limit inherent in Random Slicing, but a choice\nwe made to speed up implementation. As Scott's article points out, Random Slicing\nmatches your keyspace to your storage capacity, but that is it! It doesn't\nmanage replica placement. More complex replica placement policies are coming,\nread Scott's article \ud83d\ude09"),(0,o.kt)("p",null,"In short, Random Slicing appears very simple, map capacity to the range\n0-1, and assign values to slices in that range. Cut-Shift is a great\nway to efficiently carve new smaller, partitions from slices of larger\nones, and coalesce smaller slices into larger partitions when HyDRA\nscales up or down."),(0,o.kt)("p",null,"Each storage node uses the Random Slicing partitioning information to\ndecide if it needs to store documents from any given\ntransaction. If the Random Slicing map says that Server One owns\nDocuments in the first Partition, then for each transaction Server One will\nstore Documents whose Ids hash to the first partition."),(0,o.kt)("h5",{id:"interval-maps---missed-transactions---backfill"},"Interval Maps - Missed Transactions - Backfill"),(0,o.kt)("p",null,"Each storage nodes keeps a local data structure, stored durably and\natomically with the document data, that records what transactions the\nnode has observed. The structure is called the ",(0,o.kt)("inlineCode",{parentName:"p"},"IntervalMap"),", and\nrepresents what has been observed, in what slices of the keyspace."),(0,o.kt)("p",null,'For example, if a server is responsible for an interval of the\nkeyspace that represents the first third of the keyspace, the server\n"splices" the observed transactions into the ',(0,o.kt)("inlineCode",{parentName:"p"},"IntervalMap")," at that\ninterval."),(0,o.kt)("p",null,"Imagine Server One is responsible for Interval One, it receives transactions\n1..=100 from the log, it adds the data from those transactions to a local write\ntransaction with RocksDb. Then it splices the information into the IntervalMap,\nthat it has seen a block of transactions from 1..=100. We now say that the\n",(0,o.kt)("inlineCode",{parentName:"p"},"base")," for Interval One is ",(0,o.kt)("inlineCode",{parentName:"p"},"100"),". Now the server stores this updated\n",(0,o.kt)("inlineCode",{parentName:"p"},"IntervalMap")," with the data in a write transaction to RocksDb."),(0,o.kt)("p",null,"Next the server receives transaction ",(0,o.kt)("inlineCode",{parentName:"p"},"150..=200")," from the log. Clearly the\nserver can detect that it has somehow missed transaction ",(0,o.kt)("inlineCode",{parentName:"p"},"101..=149"),". The server\ncan still observe and record the data from these new transactions, and splice\nthe information into the ",(0,o.kt)("inlineCode",{parentName:"p"},"IntervalMap"),". The ",(0,o.kt)("inlineCode",{parentName:"p"},"IntervalMap")," now has a ",(0,o.kt)("inlineCode",{parentName:"p"},"base")," of\n",(0,o.kt)("inlineCode",{parentName:"p"},"100")," and a ",(0,o.kt)("inlineCode",{parentName:"p"},"detatched-range")," of ",(0,o.kt)("inlineCode",{parentName:"p"},"150..=200"),"."),(0,o.kt)("p",null,"Any server with any detached ranges can look in the Partition Map to see if it\nhas any peer replicas, and ask ",(0,o.kt)("em",{parentName:"p"},"them")," for the detached range(s). This is an\ninternal query in HyDRA. If a peer replica has some or all of the missing\ntransaction data, it will send it to the requesting server, who will splice the\nresults in the ",(0,o.kt)("inlineCode",{parentName:"p"},"IntervalMap"),", and write the data to disk. This way a Server can\nrecover any data it missed, assuming at least one replica stored that data. We\ncall this Backfill."),(0,o.kt)("p",null,"Nodes gossip their ",(0,o.kt)("inlineCode",{parentName:"p"},"IntervalMaps"),", this is how the UST is calculated, and how Backfill replicas can be chosen."),(0,o.kt)("p",null,'Read on down to "Data Loss" if you want to know how the cluster continues to make\nprogress and function in the disastrous case that all replicas miss a\ntransaction.'),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"IntervalMap"),', gossip, Backfill, UST, Read Transactions, and the GC\ntimestamp all come together to facilitate "transitions", which is how HyDRA can\nscale up and down, while remaining operational, available, and consistent.'),(0,o.kt)("h2",{id:"transitions"},"Transitions"),(0,o.kt)("p",null,"Also mentioned in Scott's article on Random Slicing is the fact that\nRandom Slicing will not tell you how, or when, to move your data\naround if you want to go from one set of partitions to another."),(0,o.kt)("p",null,"In HyDRA we have the added problem that we must at all times remain\nCausally Consistent. HyDRA manages Transitions between configurations\nby leaning on those two primitives the UST and the GC Timestamp. The\nprocess is best explained with an example."),(0,o.kt)("p",null,"Using the diagram from the Random Slicing section, a walkthrough of the transition from the 3 Partition original cluster to the target 4 partition cluster. In this case assume 2 Replicas per partition, which means adding 2 new servers to the cluster."),(0,o.kt)("p",null,"There is a Current Config, that contains the intervals  that make up the Partitions 1, 2, and 3 mapped to the replicas for those Partitions. The name ",(0,o.kt)("inlineCode",{parentName:"p"},"p1r1")," refers to the first replica of partition 1, ",(0,o.kt)("inlineCode",{parentName:"p"},"p2r2")," the second replica of partition 2, etc."),(0,o.kt)("p",null,"In the Current Config there are nodes ",(0,o.kt)("inlineCode",{parentName:"p"},"p1r1"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"p1r2"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"p2r1"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"p2r2"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"p3r1"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"p3r2"),". Two new nodes are started, (",(0,o.kt)("inlineCode",{parentName:"p"},"p4r1"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"p4r2"),"). A new Cluster Configuration is generated from the Current Configuration. This runs the Cut-Shift algorithm and produces a Next Configuration, with the partition map and intervals as-per the diagram above."),(0,o.kt)("p",null,"We store the Current Config, and the Next Config in a strongly consistent metadata store. Updating the metadata store causes the Current Config and Next Config file to be written to location on disk for each HyDRA Store node, and each node is signaled to re-read the configs."),(0,o.kt)("p",null,"The servers in ",(0,o.kt)("inlineCode",{parentName:"p"},"p1-p3")," are all in the Current Config, and the Next Config. The servers in ",(0,o.kt)("inlineCode",{parentName:"p"},"p4")," are only in the Next Config."),(0,o.kt)("p",null,"A server will consume from the log if it is in either config. Those in both configs will store data in all intervals they own in both configs. In our example each of the current config servers stores a subset of the current sub-interval of its current ownership in the next config. The new servers in `p4`` start to consume from the log at once, and gossip to all their peers in both configs."),(0,o.kt)("h3",{id:"backfill-again"},"Backfill, again"),(0,o.kt)("p",null,"For example, we start the new servers when the oldest transaction available on the log is ",(0,o.kt)("inlineCode",{parentName:"p"},"Txn Id 1000"),". They must Backfill from ",(0,o.kt)("inlineCode",{parentName:"p"},"0-1000")," from the owners of their intervals in the Current Configuration. They use the Current Configuration to calculate those owners, and the ",(0,o.kt)("inlineCode",{parentName:"p"},"IntervalMap"),"s from gossip to pick an owner to query for data no longer on the log. Recall that the UST is calculated from the ",(0,o.kt)("inlineCode",{parentName:"p"},"base")," of the ",(0,o.kt)("inlineCode",{parentName:"p"},"IntervalMap")," but these new servers (only part of the new config) do not contribute to the UST until they have Backfilled."),(0,o.kt)("h3",{id:"routing-and-a-ust-per-configuration"},"Routing, and a UST per-Configuration"),(0,o.kt)("p",null,"In the section on UST we described a scalar value, the Transaction Timestamp. In reality this value is a pair of the ",(0,o.kt)("inlineCode",{parentName:"p"},"ConfigurationId")," and the UST. The ",(0,o.kt)("inlineCode",{parentName:"p"},"ConfigurationId")," rises monotonically, the initial Configuration being ",(0,o.kt)("inlineCode",{parentName:"p"},"ConfigId 1"),", the second ",(0,o.kt)("inlineCode",{parentName:"p"},"ConfigId 2"),", etc."),(0,o.kt)("p",null,"This allows us to calculate a UST per-configuration. Before we began the transition the UST was ",(0,o.kt)("inlineCode",{parentName:"p"},"(1, 1000)"),". The UST may never go backwards (that would break Causal Consistency). After starting the new servers and notifying nodes about the Next Config, the UST in the Current Config is ",(0,o.kt)("inlineCode",{parentName:"p"},"(1, 1000)")," and in the Next Config is ",(0,o.kt)("inlineCode",{parentName:"p"},"(2, 0)"),". During this period of transition the nodes in `p4`` cannot be routed to for querying. Only nodes in the Current Config can coordinate queries, and these nodes decide what Configuration to use for Routing based on the USTs in each of the Current and Next Config. We call this the Routing Config. It is calculated. And like everything else in HyDRA, it progresses monotonically upwards."),(0,o.kt)("p",null,"After the new nodes have Backfilled, and after some period of gossip, the UST in the Next Config arrives at a value that is ",(0,o.kt)("inlineCode",{parentName:"p"},">=")," the UST in the current config* the servers in the Current Config will begin to Route queries using the Next Config. Recall that nodes gossip a GC timestamp that is based on active Read Transactions. A Read Transaction is identified by the Timestamp at which it began. For example ",(0,o.kt)("inlineCode",{parentName:"p"},"(1, 1000)")," is a Read Transaction that began at UST 1000 in the Current Configuration. When all the replicas in the Current Configuration are Routing with the next configuration, (i.e. the Cluster GC timestamp is in the Next Configuration, ",(0,o.kt)("inlineCode",{parentName:"p"},"(2, 1300)")," for example) the Transition is complete. Any of the nodes can store the Next Config into the Strongly Consistent metadata store as the Current Config. Each node is signaled, and eventually all will have a Current Config with ",(0,o.kt)("inlineCode",{parentName:"p"},"ConfigId 2"),", and will forget metadata related to ",(0,o.kt)("inlineCode",{parentName:"p"},"ConfigId 1"),". Furthermore, Garbage Collection will ensure that replicas drop data that they no longer own."),(0,o.kt)("p",null,"Throughout the transition, writes are processed, queries are executed, and the normal monotonic progress of the Cluster's UST and GC timestamp ensure that the new nodes can begin to store data at once, and will be used for query capacity as soon as they support Causally Consistent view of the data."),(0,o.kt)("p",null,"*(there are details elided here about how we ensure that the Next Config makes progress and catches up with the current, whilst ensuring the cluster still moves forward)"),(0,o.kt)("h2",{id:"handling-failure"},"Handling Failure"),(0,o.kt)("p",null,"There are many failure scenarios in any distributed system. HyDRA leans heavily on a durable transaction log for many failure scenarios, and replicated copies of data for many others. Safety in HyDRA (bad things never happen) has been discussed at length above, in UST, and Transitions, and how Causally Consistent reads occur. Liveness, however, depends on every replica contributing to the UST. The UST (and GC timestamp) are calculated from gossip from ",(0,o.kt)("em",{parentName:"p"},"every")," node. If any node is down, partitioned by the network, slow, or in some other way broken, it impacts the progress of the cluster. Yes, HyDRA can still accept writes, and serve (ever staler) reads, but the UST won't rise, Transitions won't finish and GC will stall (leading to many versions on disk.)"),(0,o.kt)("p",null,"It is possible in future that we make some changes to how the UST is calculated, and use a quorum of nodes from a partition, or the single highest maximum transaction from a partition. The trade-off being that query routing becomes more complex, and in the event that the node that set the UST high then becomes unavailable...something has to give in terms of consistency. These trade-offs are mutable, we can re-visit them, we favoured safety in the current iteration of the design."),(0,o.kt)("p",null,"If some nodes are keeping the UST down, and slowing or halting progress, the bad node(s) can be removed."),(0,o.kt)("h3",{id:"bad-nodes"},"Bad Nodes"),(0,o.kt)("p",null,"In this first iteration of HyDRA we have the blunt, expedient tool available to us of killing and removing a node that is bad. The process is simple. Update the Current Config to remove the offending node(s) and signal the remaining servers. They will immediately no longer route to that node, use that node in their calculations, or listen to gossip from that node. This is fine as long as at least one node in each partition of the map is left standing."),(0,o.kt)("p",null,"As soon as the offending nodes are removed the data is under-replicated. At once add replacement nodes by performing a transition as above. For example, imagine ",(0,o.kt)("inlineCode",{parentName:"p"},"p1r2")," has become unresponsive. Remove it from the Current Config, create a Next Config with a new server to take the place of ",(0,o.kt)("inlineCode",{parentName:"p"},"p1r2"),", store the configs in the Strongly Consistent metadata store, and signal the nodes. The new node will begin to consume transactions and backfill, and the UST will rise etc."),(0,o.kt)("h3",{id:"missedlost-data"},"Missed/Lost Data"),(0,o.kt)("p",null,"As described in the first Backfill section, it is possible, with a long network incident and a short log retention policy, that some transactions are missed. If all the replicas for a partition miss some intersecting subset of transactions, that data has been missed, and it is lost. This should ",(0,o.kt)("em",{parentName:"p"},"never")," happen. If it does, we don't want to throw away the HyDRA cluster, and all the good data. Progress must still be made. In this case each replica of the partition understands from the ",(0,o.kt)("inlineCode",{parentName:"p"},"IntervalMap"),"s that some transaction ",(0,o.kt)("inlineCode",{parentName:"p"},"T")," has been missed. After doing a strongly consistent read of the metadata store, to check that no server in the next config exists that may have the data, the replicas agree unilaterally to pretend that really they did store this data, and they splice it into their ",(0,o.kt)("inlineCode",{parentName:"p"},"IntervalMap"),"s. The UST rises, and progress is made."),(0,o.kt)("p",null,"It is essential to understand this is a disaster scenario, and not business as usual, but disasters happen, and they should be planned for. We do everything we can to never lose data, including a replicated durable transaction log with a long retention policy."),(0,o.kt)("h3",{id:"subscription-servers"},"Subscription Servers"),(0,o.kt)("p",null,"These are soft-state servers that act as Peers to Ditto SDK devices. They speak\nthe Ditto replication protocol. SDK devices connect to the subscription server,\nand based on their subscription, the SubscriptionServer will replicate data.\nTaking from the SDK device data that HyDRA has not seen, and sending to the SDK\ndevice data HyDRA has that the SDK device subscribes to, and has not seen."),(0,o.kt)("p",null,"Subscription Servers also provide an HTTP API for non-SDK clients."),(0,o.kt)("h4",{id:"document-cache"},"Document Cache"),(0,o.kt)("p",null,"In order to not be required to query HyDRA for all data requested by SDK peers, all the time, the Subscription Server maintains a sophisticated, causally consistent, in-memory cache of a data. The data it chooses to cache is based on the Subscriptions of the SDK devices connected to it. By routing devices to a Subscription Server by AppId, we increase the likelihood that the devices have an overlapping set of Subscriptions and share common data in the cache."),(0,o.kt)("p",null,'The document cache takes data from the mesh clients and puts it on the log as transactions. It also consumes from the log, so that it can keep the data in the cache up to date, without querying HyDRA. Any documents that it observes on the log, that are potentially of interest to the cache, must first be "Upqueried" from HyDRA to populate the cache. As a cache becomes populated Upqueries decrease in size and number.'),(0,o.kt)("p",null,"As clients disconnect, if any data is no long required in the cache, it is eventually garbage collected away."),(0,o.kt)("h2",{id:"what-next"},"What Next?"),(0,o.kt)("p",null,"HyDRA is approaching beta. Some customers are already putting production workloads into HyDRA. HyDRA is far from done. We'd like to focus on stability, performance, and features. As well as those mentioned above (gossip, sophisticated resourcing/partitioning) we have the following in progress."),(0,o.kt)("h3",{id:"cdc"},"CDC"),(0,o.kt)("p",null,"Completing the cycle of data in HyDRA is CDC (",(0,o.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Change_data_capture"},"Change Data Capture"),"). Work in progress where each transaction produces a Change Data Message containing the type of change (eg insert, delete, update) and the details of the change. CDC is a way for customer to react to data changes that occur from the mesh or elsewhere, or even to keep external legacy databases in sync with HyDRA."),(0,o.kt)("p",null,"Data from CDC will first be available via webhooks. Developers can register HTTP endpoints where HyDRA will deliver data change events that match a defined query - similar to how Ditto Mesh SDKs ",(0,o.kt)("inlineCode",{parentName:"p"},"observe")," queries to react to data changes. "),(0,o.kt)("p",null,'This will enable delivery of data within HyDRA into other systems or to build server-side logic that reacts to data change events - such as performing data aggregations that write back into HyDRA or to trigger an email to a user based off an event from a Ditto Mesh SDK. These data change events fit into "serverless" patterns and will work with any "functions-as-a-service" (FaaS) systems, such as AWS Lambda or others.'),(0,o.kt)("p",null,"Care is being taken to ensure the delivery of these events are reliable. Endpoints will be able to persist a unique marker that corresponds to the event, and later restart events from that same marker onward so that events are not missed during periods of interruption."),(0,o.kt)("h3",{id:"time-series"},"Time Series"),(0,o.kt)("p",null,'In addition to storing mutable Documents backed by CRDTs, HyDRA recently added support for immutable Time Series data. We have a basic Time Series API in HyDRA and we\'re adding Time Series to the SDK. Developers can utilize these APIs to match data in their application, such as blending Documents from a companion mobile app alongside sensor Time Series data produced via an embedded or "IoT" device.  Watch this space.'),(0,o.kt)("h3",{id:"http-api"},"HTTP API"),(0,o.kt)("p",null,"Although we have a rudimentary HTTP API, we expect this area to grow and improve to make it simpler for non-SDK clients to use HyDRA. At present you can insert, query, and delete docs. Expect transactions and updates in the future."),(0,o.kt)("h3",{id:"eventual-consistency-mode"},"Eventual Consistency Mode"),(0,o.kt)("p",null,'Multi-model databases are becoming more popular. We\'d like to add an "Eventual Consistency Mode" to HyDRA, where the overhead of Causal Consistency is not needed.'),(0,o.kt)("h3",{id:"improved-queries-and-indexing"},"Improved Queries and Indexing"),(0,o.kt)("p",null,"We'd like to expand the types of queries offered, such as cross-collection joins, in addition, to offer flexible indexing for improved query performance."),(0,o.kt)("h3",{id:"summary"},"Summary"),(0,o.kt)("p",null,"HyDRA is multi-tenant, distributed, causally consistent CRDT database,\nbuilt by Ditto, and operated by Ditto. When creating HyDRA our view\nwas always towards which architectural fundamentals give us the\nprimitives we need to support the properties we want."),(0,o.kt)("p",null,"HyDRA is under active development. We expect to expand and iterate and to\noptimize. HyDRA has been written as software that Ditto can ship and support\ncustomers deploying and running themselves, running it as a service at Ditto is\nonly the first part of the story."))}u.isMDXComponent=!0},2928:function(e,t,a){t.Z=a.p+"assets/images/cutshift-17d225642fc7e916bda2ae752449ef38.png"},8059:function(e,t,a){t.Z=a.p+"assets/images/diagram-0668f99c916da479bb921499c46fee1c.png"}}]);